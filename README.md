# ðŸ›¡ï¸ RiskMetric â€” Algorithmic Fraud Detection & Spatial-Temporal Risk Modeling

> A zero-cost, high-performance **Trust Engine** that identifies complex fraud archetypes across **1M+ synthetic banking transactions** with sub-second inference speed.

![Python](https://img.shields.io/badge/Python-3.11+-blue?logo=python)
![DuckDB](https://img.shields.io/badge/DuckDB-OLAP-yellow?logo=duckdb)
![dbt](https://img.shields.io/badge/dbt--core-Pipeline-orange?logo=dbt)
![Streamlit](https://img.shields.io/badge/Streamlit-Dashboard-red?logo=streamlit)

---

## Architecture â€” Medallion System

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GitHub Actions CI/CD                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  BRONZE  â”‚      SILVER      â”‚     GOLD      â”‚   INTERFACE   â”‚
â”‚          â”‚                  â”‚               â”‚               â”‚
â”‚ generatorâ”‚ Impossible Travelâ”‚ Risk Scores   â”‚  Streamlit    â”‚
â”‚   .py    â”‚ (Haversine/GPS)  â”‚ (Composite)   â”‚  Dashboard    â”‚
â”‚          â”‚                  â”‚               â”‚               â”‚
â”‚ 1M+ raw  â”‚ Velocity Spikes  â”‚ User Risk     â”‚  Real-time    â”‚
â”‚ records  â”‚ (60s windows)    â”‚ Profiles      â”‚  Watchdog     â”‚
â”‚          â”‚                  â”‚               â”‚               â”‚
â”‚ Parquet  â”‚ Behavioral Drift â”‚ Fraud         â”‚  SQL Explorer â”‚
â”‚          â”‚ (Z-Score/30d MA) â”‚ Attribution   â”‚               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Faker   â”‚     DuckDB       â”‚   dbt-core    â”‚   Plotly      â”‚
â”‚  PyArrow â”‚   + dbt macros   â”‚  + dbt tests  â”‚   + Geo Maps  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Fraud Archetypes Detected

| Archetype | Method | Threshold |
|---|---|---|
| **Impossible Travel** | Haversine distance + ground speed between consecutive transactions | > 500 mph |
| **Velocity Spikes** | 60-second sliding window transaction count | â‰¥ 10 txns / 60s |
| **Behavioral Drift** | Z-Score against 30-day rolling average | \|Z\| > 3Ïƒ |

## Tech Stack ($0 Cost)

- **Engine**: DuckDB â€” vectorized, in-process OLAP
- **Storage**: Apache Parquet â€” columnar, compressed
- **Pipeline**: dbt-core â€” modular, tested, version-controlled SQL
- **Orchestration**: GitHub Actions â€” CI/CD on every push
- **Interface**: Streamlit + Plotly â€” executive-level dashboard

## Quick Start

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Run Full Pipeline (Bronze â†’ Silver â†’ Gold)

```bash
python run_pipeline.py
```

This will:
- Generate 1M+ synthetic transactions with injected fraud
- Run all dbt models (Silver enrichment + Gold aggregation)
- Run dbt data integrity tests
- Export Gold tables to Parquet
- Generate dbt documentation and data lineage graph
- Print a summary report with model evaluation metrics

### 3. Launch Dashboard

```bash
streamlit run dashboard/app.py
```

### 4. View Data Lineage (dbt Docs)

```bash
dbt docs serve --profiles-dir riskmetric --project-dir riskmetric
```

### Manual Steps (Optional)

```bash
# Generate data only
python generator.py

# Run dbt models only (with full refresh for incremental models)
cd riskmetric
dbt run --profiles-dir . --project-dir . --full-refresh
dbt test --profiles-dir . --project-dir .
dbt docs generate --profiles-dir . --project-dir .
```

## Project Structure

```
Banking Sentinel/
â”œâ”€â”€ generator.py                    # Bronze: 1M+ synthetic transaction generator
â”œâ”€â”€ run_pipeline.py                 # Full pipeline orchestrator
â”œâ”€â”€ config.yml                      # Centralized thresholds & calibration config
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ data/                           # Generated data (gitignored)
â”‚   â”œâ”€â”€ raw_transactions.parquet    # Bronze layer
â”‚   â”œâ”€â”€ user_profiles.parquet       # Bronze layer
â”‚   â””â”€â”€ riskmetric.duckdb           # DuckDB warehouse
â”œâ”€â”€ riskmetric/                     # dbt project
â”‚   â”œâ”€â”€ dbt_project.yml             # dbt config + vars for threshold calibration
â”‚   â”œâ”€â”€ profiles.yml
â”‚   â”œâ”€â”€ macros/
â”‚   â”‚   â””â”€â”€ haversine.sql           # Haversine distance macro
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ staging/                # Raw â†’ Staged
â”‚       â”‚   â”œâ”€â”€ stg_transactions.sql
â”‚       â”‚   â””â”€â”€ stg_user_profiles.sql
â”‚       â”œâ”€â”€ silver/                 # Fraud detection (incremental)
â”‚       â”‚   â”œâ”€â”€ silver_impossible_travel.sql
â”‚       â”‚   â”œâ”€â”€ silver_velocity_spikes.sql
â”‚       â”‚   â””â”€â”€ silver_behavioral_drift.sql
â”‚       â””â”€â”€ gold/                   # Curated risk models
â”‚           â”œâ”€â”€ gold_risk_scores.sql
â”‚           â”œâ”€â”€ gold_user_risk_profiles.sql
â”‚           â”œâ”€â”€ gold_fraud_attribution.sql
â”‚           â”œâ”€â”€ gold_model_evaluation.sql       # Precision/Recall/F1
â”‚           â””â”€â”€ gold_threshold_calibration.sql  # Threshold sensitivity
â”œâ”€â”€ dashboard/
â”‚   â””â”€â”€ app.py                      # Streamlit dashboard (8 pages)
â””â”€â”€ .github/workflows/
    â””â”€â”€ pipeline.yml                # CI/CD automation
```

## Risk Scoring

The composite risk score (0â€“100) is a weighted sum of detected fraud signals:

| Signal | Weight | Rationale |
|---|---|---|
| Impossible Travel | 40 pts | Strongest signal â€” rarely a false positive at >500 mph |
| Velocity Spike | 35 pts | Card testing precursor â€” high urgency for blocking |
| Behavioral Drift | 25 pts | Spending anomalies â€” higher FP rate, requires human review |

**Risk Tiers**: CRITICAL (â‰¥60) Â· HIGH (â‰¥35) Â· MEDIUM (â‰¥25) Â· LOW (<25)

All weights and tier cutoffs are externalized as dbt variables in `dbt_project.yml` and documented in `config.yml`, enabling data-driven recalibration without code changes.

## Model Evaluation

The pipeline computes full classification metrics against ground-truth labels:

| Metric | Description |
|---|---|
| **Precision** | TP / (TP + FP) â€” how many flags are real fraud |
| **Recall** | TP / (TP + FN) â€” how much fraud is caught |
| **F1-Score** | Harmonic mean of Precision and Recall |
| **False Positive Rate** | FP / (FP + TN) â€” operational noise level |
| **Confusion Matrix** | Full TP/FP/FN/TN breakdown per archetype |

The `gold_threshold_calibration` model evaluates detection performance across a range of thresholds per archetype, producing precision-recall tradeoff curves for optimal operating point selection.

## Calibration Methodology

1. **Initial weights** set by domain expertise (fraud analyst heuristics)
2. **Threshold sensitivity analysis** via `gold_threshold_calibration` â€” tests speed thresholds (200â€“800 mph), velocity counts (3â€“20 txns), and Z-Score bounds (1.5â€“5.0Ïƒ)
3. **Validation** against labeled ground truth using `gold_model_evaluation` â€” reports precision, recall, and F1 per archetype
4. **Production path**: weights would be tuned via logistic regression coefficients or gradient-boosted model feature importances; periodic recalibration (quarterly) as fraud patterns evolve

## Incremental Processing

Silver-layer models use dbt's `incremental` materialization strategy:
- On first run: full table build
- On subsequent runs: only processes transactions newer than `MAX(transaction_timestamp)` in the existing table
- Use `--full-refresh` flag to force a complete rebuild

This mirrors production patterns where millions of daily transactions are processed incrementally rather than rebuilt from scratch.

## Dashboard Pages

| Page | Purpose |
|---|---|
| **Overview** | Executive KPIs, fraud by archetype, timeline, accuracy |
| **Impossible Travel** | Speed distribution, geo map, flagged transactions |
| **Velocity Spikes** | Burst analysis, amount patterns |
| **Behavioral Drift** | Z-Score distribution, amount vs rolling average |
| **User Profiles** | Per-user risk aggregation, tier filtering |
| **Model Evaluation** | Precision/Recall/F1, confusion matrix, calibration curves |
| **Triage Queue** | Alert case management with disposition workflow |
| **Raw Explorer** | Ad-hoc SQL queries against DuckDB warehouse |

## Known Limitations & Future Work

- **Self-labeled data**: Fraud is injected by the same system that detects it, creating circular validation. A production system would use externally labeled data or blind holdout sets.
- **No temporal train/test split**: All evaluation is in-sample. Production deployment would require forward-looking validation on unseen future data.
- **Static thresholds**: Current thresholds are fixed. A production system would use adaptive thresholds that adjust to evolving fraud patterns (concept drift).
- **Scale ceiling**: DuckDB handles 1M+ records locally with ease. At 1B+ records, consider partitioned Parquet, incremental dbt models with date-based partitioning, and migration to cloud OLAP (Snowflake/BigQuery).
- **No real-time stream**: Current architecture is batch-oriented. Real-time fraud detection would require a streaming layer (Kafka + Flink) feeding into the same model logic.

## Resume Bullet (STAR Format)

> **RiskMetric: Algorithmic Fraud Intelligence** | DuckDB, dbt, Python, GitHub Actions
>
> Engineered a production-grade risk modeling system processing 1M+ synthetic records, utilizing DuckDB for vectorized spatial-temporal analysis and dbt for modular, incremental pipeline architecture. Developed a custom Haversine-based detection algorithm to identify "Impossible Travel" fraud, with threshold calibration analysis achieving optimized precision-recall tradeoffs across three distinct fraud archetypes. Automated the full lifecycle via GitHub Actions (CI/CD), incorporating 30+ dbt-tests for data integrity, model evaluation metrics (Precision/Recall/F1), and deploying a live Streamlit dashboard with alert triage workflow for real-time risk attribution.
